/**
 * Mermaid diagram templates for all projects
 * Architecture & Workflow diagrams in Mermaid syntax
 */

export const MERMAID_DIAGRAMS = {
  'social-media-sentiment': {
    architecture: `graph LR
      A["ğŸ“± Social Apps<br/>Twitter, TikTok, etc"] -->|Stream| B["ğŸ“¨ Kafka<br/>Message Queue"]
      B -->|Ingest| C["âš¡ Spark Cluster<br/>Real-time Processing"]
      C -->|Process| D["ğŸ§  BERT Microservices<br/>Sentiment & Topic"]
      D -->|Store| E["ğŸ’¾ Feature Store<br/>Historical Features"]
      D -->|Alert| F["ğŸ”” Alert Service<br/>Thresholds & Rules"]
      F -->|Send| G["ğŸ“Š Analytics Dashboard<br/>Real-time Insights"]
      E -->|Feedback| C`,
    workflow: `graph TD
      A["ğŸ“¥ Raw Stream<br/>Multi-language Posts"] --> B["ğŸŒ Language Detection"]
      B --> C["ğŸ“ Tokenization<br/>Preprocessing"]
      C --> D["ğŸ§  BERT Inference<br/>Sentiment & Topic"]
      D --> E["ğŸ“Š Aggregate Metrics<br/>Rolling Windows"]
      E --> F["ğŸš¨ Alert Routing<br/>Threshold Logic"]
      F --> G["ğŸ“ˆ Visualization<br/>Real-time Dashboard"]`
  },
  
  'customer-churn-intelligence': {
    architecture: `graph LR
      A["ğŸ—„ï¸ Data Lake<br/>Customer Events"] --> B["ğŸ”„ Airflow ETL<br/>Orchestration"]
      B -->|Transform| C["ğŸ“¦ Feature Store<br/>Historical Features"]
      C -->|Train| D["ğŸ§ª MLflow Registry<br/>Model Versioning"]
      D --> E["ğŸ¤– Ensemble Models<br/>XGBoost + CatBoost"]
      E --> F["âš™ï¸ FastAPI Scorer<br/>Live Predictions"]
      F --> G["ğŸ“Š Analytics Dashboard<br/>Risk Insights"]
      G -->|Feedback| B`,
    workflow: `graph TD
      A["ğŸ“¥ Ingest Data<br/>Raw Customer Events"] --> B["ğŸ§¹ Clean & Encode<br/>Feature Engineering"]
      B --> C["ğŸ“š Train & Validate<br/>Cross-validation"]
      C --> D["âœ… Register Model<br/>MLflow Registry"]
      D --> E["ğŸš€ Deploy<br/>FastAPI Service"]
      E --> F["ğŸ“¡ Monitor Drift<br/>Performance Tracking"]
      F -->|Trigger| G["ğŸ”„ Retrain Cycle"]`
  },
  
  'brain-tumor-classification': {
    architecture: `graph LR
      A["ğŸ¥ PACS/MRI Storage<br/>Medical Images"] --> B["ğŸ–¼ï¸ Preprocessing<br/>Normalization & Augment"]
      B -->|Process| C["ğŸ§  CNN Cluster<br/>ResNet/EfficientNet"]
      C -->|Explain| D["ğŸ” Grad-CAM++<br/>Explainability"]
      D -->|Generate| E["ğŸ“‹ Report Generator<br/>PDF Creation"]
      E -->|Send| F["ğŸ‘¨â€âš•ï¸ Clinician Portal<br/>Review Interface"]`,
    workflow: `graph TD
      A["ğŸ“¤ MRI Import<br/>DICOM Files"] --> B["âš™ï¸ Normalize<br/>Data Preprocessing"]
      B --> C["ğŸ§  CNN Inference<br/>Tumor Detection"]
      C --> D["ğŸ”¥ Grad-CAM Heatmap<br/>Attention Maps"]
      D --> E["âœ”ï¸ QA Review<br/>Manual Verification"]
      E --> F["ğŸ“„ EMR Export<br/>Clinical Integration"]`
  },
  
  'financial-fraud-detection': {
    architecture: `graph LR
      A["ğŸ’³ Payment Gateways<br/>Transaction Stream"] -->|Event| B["ğŸ“¨ Kafka Backbone<br/>Event Processing"]
      B -->|Feature Eng| C["ğŸ”¢ Feature Store<br/>Behavior Patterns"]
      C -->|Score| D["ğŸ¤– Autoencoder<br/>Reconstruction Error"]
      C -->|Score| E["ğŸŒ³ Isolation Forest<br/>Anomaly Detection"]
      D & E -->|Ensemble| F["âš¡ Scoring Service<br/>Real-time"]
      F -->|Alert| G["ğŸš¨ SOC Dashboard<br/>Analyst Queue"]`,
    workflow: `graph TD
      A["ğŸ’¸ Ingest Transaction<br/>Real-time Stream"] --> B["âš™ï¸ Normalize<br/>Feature Extraction"]
      B --> C["ğŸ“Š Autoencoder<br/>Reconstruction Error"]
      C --> D["ğŸ¯ Threshold Logic<br/>Anomaly Detection"]
      D --> E["ğŸ“‹ Case Creation<br/>Alert Generation"]
      E --> F["ğŸ‘ï¸ Analyst Review<br/>Feedback Loop"]`
  },
  
  'yolov8-inference-engine': {
    architecture: `graph LR
      A["ğŸ“¹ IP Cameras<br/>WebRTC Streams"] --> B["ğŸ”„ Preprocessor<br/>Resizing & Normalization"]
      B -->|Stream| C["ğŸ¯ YOLOv8 Nodes<br/>Object Detection"]
      C -->|Optimize| D["âš¡ TensorRT<br/>Hardware Acceleration"]
      D -->|Stream| E["ğŸŒ Gateway<br/>RTSP/HLS Output"]
      E -->|Display| F["ğŸ“Š Monitoring<br/>Live Dashboard"]`,
    workflow: `graph TD
      A["ğŸ“¹ Capture Frame<br/>Camera Input"] --> B["ğŸ”€ Decode<br/>Codec Parsing"]
      B --> C["ğŸ–¼ï¸ Preprocess<br/>Resize & Normalize"]
      C --> D["ğŸ¯ YOLOv8 Detection<br/>Model Inference"]
      D --> E["ğŸ§¹ NMS Post-process<br/>Deduplication"]
      E --> F["âœï¸ Draw Boxes<br/>Annotation"]
      F --> G["ğŸ“¤ Stream/Archive<br/>Output"]`
  },
  
  'hybrid-recommendation-engine': {
    architecture: `graph LR
      A["ğŸ‘¤ User Events<br/>Click & Rating"] -->|Store| B["ğŸ’¾ Feature Store<br/>User-Item Pairs"]
      B -->|Embed| C["ğŸ§  Embedding Layer<br/>Latent Vectors"]
      C -->|Index| D["âš¡ FAISS Index<br/>Vector Search"]
      D -->|Retrieve| E["ğŸ¯ Candidate Pool<br/>Fast Recall"]
      E -->|Rank| F["ğŸ“Š NCF + MF Ranker<br/>Neural Reranking"]
      F -->|Diversify| G["ğŸ Final Selection<br/>Diversification"]
      G --> H["ğŸš€ Serve API<br/>Real-time Recs"]`,
    workflow: `graph TD
      A["ğŸ‘¤ Collect Signals<br/>User Interactions"] --> B["ğŸ“¦ Embed Users/Items<br/>Latent Vectors"]
      B --> C["ğŸ” Approximate NN<br/>FAISS Search"]
      C --> D["ğŸ”„ Rerank<br/>NCF + MF"]
      D --> E["ğŸ¯ Diversify<br/>Coverage Optimization"]
      E --> F["ğŸ“¤ Serve<br/>Recommendations"]
      F -->|Feedback| G["ğŸ“Š Feedback Loop<br/>Model Updates"]`
  },
  
  'demand-forecasting-pipeline': {
    architecture: `graph LR
      A["ğŸª Retail Data<br/>Sales History"] -->|ETL| B["âš¡ PySpark<br/>Big Data Processing"]
      B -->|Features| C["ğŸ“¦ Feature Store<br/>Time-Series Features"]
      C -->|Train| D["ğŸ“ˆ Prophet<br/>Decomposition"]
      C -->|Train| E["ğŸŒ³ XGBoost<br/>Gradient Boosting"]
      D & E -->|Ensemble| F["ğŸ¯ Forecast Service<br/>Predictions"]
      F -->|Results| G["ğŸ“Š BI Dashboards<br/>Actionable Insights"]`,
    workflow: `graph TD
      A["ğŸ“¥ Ingest Sales<br/>SKU-level Data"] --> B["ğŸ§¹ Clean & Aggregate<br/>Time-Series"]
      B --> C["âš™ï¸ Feature Engineering<br/>Seasonality & Trends"]
      C --> D["ğŸ“š Train Prophet<br/>Decomposition Model"]
      C --> E["ğŸ“š Train XGBoost<br/>Gradient Boosting"]
      D & E -->|Combine| F["ğŸ“Š Ensemble Forecast<br/>Confidence Intervals"]
      F --> G["âœ”ï¸ Validate<br/>Backtest"]
      G --> H["ğŸ“ˆ Publish<br/>Dashboard Updates"]`
  },
  
  'resume-parser': {
    architecture: `graph LR
      A["ğŸ“„ Resume Upload<br/>PDF/DOCX"] -->|OCR| B["ğŸ§¹ Cleaning<br/>Text Extraction"]
      B -->|Parse| C["ğŸ·ï¸ NER Module<br/>BERT + spaCy"]
      C -->|Extract| D["ğŸ’¼ Skill Graph<br/>Skill Normalization"]
      D -->|Match| E["ğŸ” Matching Engine<br/>Job Description Match"]
      E -->|Score| F["ğŸ‘¥ Recruiter Dashboard<br/>Ranked Candidates"]`,
    workflow: `graph TD
      A["ğŸ“¥ Upload Resume<br/>Any Format"] --> B["ğŸ“¤ OCR Extraction<br/>Text Parsing"]
      B --> C["ğŸ§¹ Clean Text<br/>Normalization"]
      C --> D["ğŸ·ï¸ Extract Entities<br/>Roles, Skills, Edu"]
      D --> E["ğŸ¯ Normalize Skills<br/>Knowledge Graph"]
      E --> F["ğŸ”— Match Job Desc<br/>Similarity Scoring"]
      F --> G["ğŸ“Š Generate Score<br/>Candidate Ranking"]`
  },
  
  'cloud-data-warehouse': {
    architecture: `graph LR
      A["ğŸ“Š Data Sources<br/>APIs, Databases"] -->|Ingest| B["ğŸŒŠ Cloud Storage<br/>Data Lake"]
      B -->|Transform| C["âš™ï¸ dbt Pipeline<br/>Transformation"]
      C -->|Load| D["ğŸ¢ Cloud DW<br/>Snowflake/BigQuery"]
      D -->|Query| E["ğŸ“ˆ BI Tools<br/>Analytics"]
      E -->|Results| F["ğŸ‘¥ Stakeholders<br/>Insights"]`,
    workflow: `graph TD
      A["ğŸ”Œ Source Connect<br/>APIs & Databases"] --> B["ğŸ“¥ Extract<br/>Batch or Stream"]
      B --> C["ğŸŒŠ Stage Data<br/>Cloud Storage"]
      C --> D["âš™ï¸ Transform<br/>dbt Models"]
      D --> E["ğŸ—„ï¸ Load DW<br/>Dimension & Facts"]
      E --> F["ğŸ” Data Quality<br/>Validation"]
      F --> G["ğŸ“Š Serve Data<br/>Analytics"]`
  },
  
  'quantum-blood-group': {
    architecture: `graph LR
      A["ğŸ©¸ Blood Sample<br/>Image Input"] --> B["ğŸ”„ Preprocess<br/>Normalization"]
      B -->|Classical| C["ğŸ§  CNN<br/>Feature Extraction"]
      C -->|Quantum| D["âš›ï¸ VQC Layer<br/>Qiskit/PennyLane"]
      D -->|Classical| E["ğŸ¯ Classifier<br/>Output Layer"]
      E -->|Result| F["ğŸ“Š Insights<br/>Visualization"]`,
    workflow: `graph TD
      A["ğŸ“¸ Capture Image<br/>Blood Sample"] --> B["âš™ï¸ Normalize<br/>Preprocessing"]
      B --> C["ğŸ§  CNN Extract<br/>Feature Maps"]
      C --> D["âš›ï¸ Quantum Layer<br/>VQC Inference"]
      D --> E["ğŸ“Š Post-process<br/>Probability"]
      E --> F["âœ”ï¸ Evaluate<br/>Classification"]`
  }
};
